---
title: "Chap. 5 - Fraudulent Reviews Identification and Analysis"
header-includes:
   - \usepackage[default]{sourcesanspro}
mainfont: sourcesanspro
author: "Max"
date: "`r format(Sys.time(), '%B %Y')`"
output: pdf_document
params: 
  geo: Montreal
  proj: 32618
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)

```

```{r data, include = FALSE}
library(here)
source(here("R", "01_source.R"))

load(here("output", "str_processed.Rdata"))
load(here("output", "review_processed.Rdata"))
load(here("output", "fake_reviews_uc.Rdata"))

geography <- cancensus::get_census(dataset = "CA16",
                      regions = list(CSD = c(2466023)),
                      level = "CSD",
                      geo_format = "sf") %>% 
  st_transform(params$proj)

```
# Review overview
```{r review_overview, include = FALSE}

date_first_review <- 
  review %>% 
  summarize(min(date)) %>% 
  pull() %>% 
  format("%B %Y")

date_last_review <- 
  review %>% 
  summarize(max(date)) %>% 
  pull() %>% 
  format("%B %Y")

nb_reviews <- 
  review_text %>% 
  nrow() %>% 
  prettyNum(",")
  
  
```

The first review on Airbnb, in `r params$geo` is dated from `r date_first_review`. Since then, and up to `r date_last_review` in the same geography, a total of `r nb_reviews` english reviews of at least 5 words have been let on the platform. 

## Review location

Here is a heat map of all reviews let during the period mentioned above. No surprise, most of the reviews reviews were let in the central part of the city, where there is also a greater percentage of STR. 
```{r plot_reviews, echo = FALSE, warning = FALSE}

# Add a geometry to each review
review_geometry <- 
  review %>% 
  left_join(select(property, property_ID), by = "property_ID") %>% 
  st_as_sf()
  
review_geometry %>% 
  ggplot()+
  geom_sf(data = geography, color = "white")+
  # geom_sf(size = 0.01)+
  # stat_density_2d(mapping = ggplot2::aes(x = purrr::map_dbl(geometry, ~.[1]),
  #                                        y = purrr::map_dbl(geometry, ~.[2]),
  #                                        fill = stat(density)),
  #                 geom = 'tile',
  #                 contour = FALSE,
  #                 alpha = 0.5)+
  # stat_density_2d(mapping = ggplot2::aes(x = purrr::map_dbl(geometry, ~.[1]),
  #                                        y = purrr::map_dbl(geometry, ~.[2])))+
  # stat_density_2d(mapping = ggplot2::aes(x = purrr::map_dbl(geometry, ~.[1]),
  #                                         y = purrr::map_dbl(geometry, ~.[2]),
  #                                        fill = ..level..), geom = "polygon")+
  geom_hex(mapping = ggplot2::aes(x = purrr::map_dbl(geometry, ~.[1]),
               y = purrr::map_dbl(geometry, ~.[2]),
               fill = stat(density)), 
           bins = 100) +
  scale_fill_continuous(type = "viridis", 
                        limits = c(0, 0.025), #oob = scales::squish, 
                        labels = scales::percent)+
  theme_void()
  
```

# Fake review identification

```{r fake_review_identification, include = FALSE}

fake_reviews <- 
  fake_reviews_uc %>% #later, the ones from the model will be added too
  pull(review_ID)

nb_fake_reviews <- 
  length(fake_reviews) %>% #+ nrow(fake_reviews_model)
  prettyNum(",") 

nb_fake_reviews_eh <- 
  review %>% 
  filter(review_ID %in% fake_reviews,
         property_ID %in% (property %>% st_drop_geometry() %>% filter(listing_type == "Entire home/apt") %>% pull(property_ID))) %>% 
  nrow() %>% 
  prettyNum(",")

nb_fake_reviews_uc <- 
  fake_reviews_uc %>% 
  nrow() %>% 
  prettyNum(",")

# nb_fake_reviews_model <-
#   fake_reviews_model %>%
#   nrow()



```

In `r params$geo`, we identified a total of `r nb_fake_reviews` fake reviews. A two-fold operation was made to get to this number. The first step was to identify, with the digital footprint of every account on Airbnb, which review was potentially fraudulent when looking at the user-centric aspect of each of them. We decided of four different characteristics; if a review fell into at least one of those, we flagged it as potentially fraudulent.

* A review let by an account to another, both part of the same network of accounts;
* A review let in a city by an account from the same city, and let in the first month of operation (technique to alleviate the "cold start" problem);
* A review part of multiple other reviews let by the reviewer to the same one reviewee;
* A review text identical to other reviews let by the same account.

When this first step of identification was done, we identified a total of `r nb_fake_reviews_uc` potential fake reviews. 

The second step was to find the rest of the fake reviews, let by more creative fraudsters who calculate their digital footprint to not get detected. However, while it is easier to trick the system in not getting detected, falsify a review text for it to appear a hundred percent genuine needs a lot of creativity. Therefore, they must use the same textual characteristics as the previous fake reviews we identified.

Consequently, we compared the textual characteristics of fake and genuine reviews to apply a model throughout the rest of the dataset. To achieve this, we identified a second set of reviews: the ones we considered had a high potential for genuineness. A review that fell into all of these characteristics was considered as potentially genuine:

* The reviewer account does not belong to the reviewee's network of account;
* The review was let at least 6 months after the creation of the listing;
* The reviewer had at least a 3 months old account;
* The review was the only review let by the reviewer to this reviewee;
* The review text was unique throughout the platform.

Having these two sets of reviews (fake and genuine) allowed us to fit a random forest regression model based only on textual characteristics using the Linguistic Inquiry and Work Count software (LIWC) mixed with bigrams, sequences of every two adjacent words, and apply the model to the rest of the review dataset. We changed the threshold to minimize the false positive classification and keep as much of a conservative number of fake reviews, and not falsely identify genuine reviews as fake. The final model x (model characteristics here), . Using this method, we found r nb_fake_reviews_model more fake reviews, for the total of `r nb_fake_reviews` fake reviews let in `r params$geo` between `r date_first_review` and `r date_last_review`, out of which `r nb_fake_reviews_eh` were in entire home listings.

## Fake review analysis

### Fake review location

For the location of fake reviews, it is no surprise that the vast majority of them are located in the central part of the city. A notable difference with the plot showing all reviews is that there are virtually no fake reviews outside of the central part of the city.

```{r fake_review_location, echo = FALSE, warning = FALSE}

fake_reviews <- 
  fake_reviews_uc %>% #later, the ones from the model will be added too
  pull(review_ID)

review_geometry %>% 
  filter(review_ID %in% fake_reviews) %>% 
  ggplot()+
  geom_sf(data = geography, color = "white")+
  geom_hex(mapping = ggplot2::aes(x = purrr::map_dbl(geometry, ~.[1]),
               y = purrr::map_dbl(geometry, ~.[2]),
               fill = stat(density)), 
           bins = 100) +
  scale_fill_continuous(type = "viridis", 
                        limits = c(0, 0.025), oob = scales::squish,
                        labels = scales::percent)+
  theme_void()
  


```
### Fake review and commercial operations

```{r fake_review_commercial_operations, include = FALSE}

date_first_property_scrape <- 
  property %>%
  st_drop_geometry() %>% 
  summarize(min(scraped)) %>% 
  pull() %>% 
  format("%B %Y")

date_last_property_scrape <- 
  property %>%
  st_drop_geometry() %>% 
  summarize(max(scraped)) %>% 
  pull() %>% 
  format("%B %Y")

commercial_properties <- 
  daily %>% 
  filter(multi == T | FREH > 0.5) %>%
  count(property_ID) %>% 
  pull(property_ID)

commercial_fake_reviewed <- 
  review %>% 
  filter(review_ID %in% fake_reviews,
         property_ID %in% (property %>% st_drop_geometry() %>% filter(listing_type == "Entire home/apt") %>% pull(property_ID))
         # I filter in only entire home, since we use FREH. Not filtering this would bring down the % of reviews in commercial operations, because what is not an entire home is surely not positive to FREH.
         ) %>% 
  mutate(commercial = ifelse(property_ID %in% commercial_properties, T, F)) %>% 
  count(commercial) %>% 
  mutate(per = scales::percent(n/sum(n), accuracy = 0.1),
         n = prettyNum(n, ","))

```
We have information about every property listing in `r params$geo` available for web scrape between `r date_first_property_scrape` and `r date_last_property_scrape`. With these information, we identified which entire-home listing were at least once in their lifetime flagged as commercial operations (either multilisting of a frequently rented entire home listing). Out of the `r nb_fake_reviews_eh` fake reviews let in entire homes, `r commercial_fake_reviewed[2,2]` (`r commercial_fake_reviewed[2,3]`) were let in commercial listings. Fraudulent reviewing is a behavior of commercial hosts, which is in line with the literature on the subject. 

### Fake reviews impact on a listing's success

```{r fake_review_impacts, echo = FALSE, warning = FALSE}

fake_reviewed_properties <- 
  review %>% 
  filter(review_ID %in% fake_reviews) %>% 
  pull(property_ID)

rev_impacts_matrice <-
review %>% 
  filter(review_ID %in% fake_reviews) %>% 
  select(date_rev = date, property_ID, review_ID) %>% 
  left_join(select(st_drop_geometry(property), property_ID, property_created = created), 
            by = "property_ID") %>% 
  left_join(select(filter(daily, status != "B"), property_ID, date_daily = date, status), 
            by = "property_ID") %>% 
  mutate(property_created = ym(str_extract(property_created, "^.{7}")),
         date_daily = ym(str_extract(date_daily, "^.{7}"))) %>% 
  group_by(review_ID, date_rev, property_ID, property_created, date_daily) %>% 
  count(status) %>% 
  # how many months between creation of the property and the review. I'm gonna have to find a fix to when we match airbnbs properties to one another. There will be one creation date. A review will maybe be let on the 13th months after creation, but in reality it's during the first month of the new listing. 
  mutate(months_from_creation = interval(property_created, date_rev) %/% months(1)) %>% 
  # some reviews are let prior to the creation of the property!
  filter(months_from_creation >= -1) %>% 
  mutate(months_from_creation = ifelse(months_from_creation == -1, 0, months_from_creation),
         days_active = sum(n)) %>% 
  ungroup() %>% 
  filter(status == "R") %>% 
  rename(days_res = n) %>% 
  # distinct() to cancel out if multiple fake reviews have been written to one listing in the same month
  select(-review_ID) %>% 
  distinct()

rev_impacts_matrice <-
  rev_impacts_matrice %>% 
  filter(date_rev == date_daily | date_rev+months(1) == date_daily) %>% 
  mutate(group2 = case_when(
    date_rev == date_daily ~ "Month of review",
    date_rev+months(1) == date_daily ~ "Month after review"
  )) %>% 
  group_by(months_from_creation, group2) %>% 
  summarize(per_res = mean(days_res)/mean(days_active)) %>%
  ungroup() %>% 
  mutate(month = ifelse(group2 == "Month after review", months_from_creation-1, months_from_creation)) %>% 
  filter(month <= 12, month != -1)

other_properties <- 
daily %>% 
  filter(status != "B",
         !property_ID %in% fake_reviewed_properties,
         # for computing time reasons, i only take a sample
         property_ID %in% sample(commercial_properties, length(fake_reviewed_properties))) %>% 
  left_join(select(st_drop_geometry(property), property_ID, property_created = created), 
            by = "property_ID") %>% 
  mutate(property_created = ym(str_extract(property_created, "^.{7}")),
         date = ym(str_extract(date, "^.{7}"))) %>% 
  group_by(property_ID, property_created, date) %>% 
  count(status) %>% 
  mutate(months_from_creation = interval(property_created, date) %/% months(1)) %>% 
  filter(months_from_creation <= 12) %>% 
  mutate(days_active = sum(n)) %>% 
  ungroup() %>% 
  filter(status == "R") %>% 
  rename(days_res = n) %>% 
  group_by(months_from_creation) %>% 
  summarize(per_res = mean(days_res)/mean(days_active))  %>% 
  mutate(month = "Other commercial properties",
         group2 = NA)

rev_impacts_matrice %>% 
  ggplot()+
  geom_line(aes(months_from_creation, per_res, group = month, color = as.factor(month)), size = 1)+
  geom_line(data = other_properties, aes(months_from_creation, per_res, group = month), size = 0.5)+
    annotate("curve", x = 11, xend = 11.5,
           y = 0.63, yend = 0.65, lwd = 0.25,
           arrow = arrow(length = unit(0.05, "inches")))+
  geom_label(data = filter(other_properties, months_from_creation == 11), aes(months_from_creation, per_res), 
             label = "Other commercial listings", size = 2.5, nudge_y = -0.02)+
  guides(color = F)+
  geom_point(aes(months_from_creation, per_res, shape = group2, color = as.factor(month)), size = 3)+
  scale_y_continuous(labels = scales::percent)+
  scale_shape_manual(values = c(18, 17))+
  scale_x_continuous(breaks = 1:12, minor_breaks = NULL)+
  theme(legend.title = element_blank())+
  ggtitle("Impact of fake reviewing on listings' percentage of reserved active days")+
  xlab("Months from creation")+
  ylab("Percentage of reserved active days")
  


# it would be interesting to know this for every time a new listing is created (prior to ab-ab image matching).

# i should take, for every listing, the variation of percentage of reserved active days from the month of the fake review to the next. 

# Should the graph have 12 lines monitoring activity from 0 to 12 months? And then we have a greater picture of before and after fake review depending of the month.

```
Commercial listings who forge reviews have a higher percentage of reserved active days. The first two months of a listing is where the growth from month to month is the highest: listings gained reputation and with enough ratings and reviews, more reservations come. This is a normal characteristics of every listings. However, in the first two months of operation, listings who faked reviews had much higher percentage of reserved days. Fake reviews during the first two months of operation seems to be very advantageous: they alleviate the problem of cold start, when a listing has no reviews so has a harder to get reserved The distance between the fake reviewed listings' lines and the rest of the commercial listings shows the advantage of this fraudulent behavior.

There are some months where fake reviewing was followed by a decrease of the listing's percentage of reserved active days (A review in the third or fifth month of activity, for example). However, in every cases, the lines stay higher than the rest of the commercial listings. The impact of a fake review between the third and the seventh month of activity appears to be mitigated. On the eight and ninth month, a fake review has a huge impact on the the percentage of reserved active days. 

