---
title: "Chap. 5.2.1. Fake review detection"
header-includes:
   - \usepackage[default]{sourcesanspro}
mainfont: sourcesanspro
author: "Max"
date: "`r format(Sys.time(), '%B %Y')`"
output: html_document
---

```{r setup, include = FALSE}

# Function for checking file modifications
# mtime <- function(files) lapply(Sys.glob(files), function(x) file.info(x)$mtime)

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(include = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

library(here)
source(here("R", "01_source.R"))
library(caret)

property <- qread(here("output", "property.qs"), parallel::detectCores())
# daily <- qread(here("output", "daily.qs"), parallel::detectCores())
# property_nm <- qread(here("output", "property_nm.qs"), parallel::detectCores())
# matches <- qread(here("output", "matches.qs"), parallel::detectCores())
# match_groupings <- qread(here("output", "match_groupings.qs"), parallel::detectCores())
# daily_nm <- qread("output/daily_nm.qs", nthreads = 8)
# host_face_confidence_networks <- qread(here("output", "host_face_confidence_networks.qs"))
# match_groupings_diff_city <- qread(here("output", "match_groupings_diff_city.qs"), parallel::detectCores())

classified_texts <- qread(here("output", "classified_texts.qs"))
qload(here("output", "liwc_lda_model.qsm"))
review_text_pred <- qread(here("output","review_text_pred_liwc.qs"))
review <- qread(here("output", "m_review.qs"))
FREH <- qread(here("output", "FREH.qs"))

```
# 5.2.1.1 Sampling relevant reviews and fitting the model

```{r first_overview}

nb_fake <- 
classified_texts %>% 
  filter(fake == TRUE) %>% 
  nrow() %>% 
  prettyNum(",")

```

The number of potential fake reviews we identified is `r nb_fake`. 

## Info on the model

```{r model_accuracy}
# Model accuracy
model_accuracy <- mean(predictions$class==test.data$fake) %>% scales::percent(accuracy = 0.01)

```

Having these two sets of reviews (fake and genuine) allowed us to fit a model, a linear discriminant analysis (LDA), based only on textual characteristics using the Linguistic Inquiry and Work Count software (LIWC). We fitted the model on 80% of our classified reviews (either fake or genuine), which is giving a discriminant function that we can apply to every other reviews. So, the rest of the classified reviews (20%) served as a test data on which we applied this discriminant function. The predictive accuracy of the model, when applied to single reviews of our test dataset, is `r model_accuracy`. FIGURE TKTK shows the result of the model on these classified reviews. Every reviews on the left of the dashed line was predicted as being a genuine review, and on the right of it, a fake review.  

```{r model_plot, include=T}

density_lda_test <- 
lda.data %>% 
  rename(Fake = fake) %>% 
  ggplot(aes(LD1)) +
  geom_density(aes(fill = Fake, color = Fake), alpha = 0.4)+
  annotate("segment", x = 0, xend = 0,
           y = -Inf, yend = Inf, alpha =1, linetype=2) +
  annotate("segment", x = 0.2, xend = 1,
           y = 0.61, yend = 0.6, lwd = 0.25,
           arrow = arrow(length = unit(0.05, "inches"))) +
  annotate("text", x = 2, y = 0.55,
           label = "Predicted as fake")+
  annotate("segment", x = -0.2, xend = -1,
           y = 0.61, yend = 0.6, lwd = 0.25,
           arrow = arrow(length = unit(0.05, "inches"))) +
  annotate("text", x = -2, y = 0.55,
           label = "Predicted as genuine")+
  scale_fill_manual(values=gp_duo4)+
  scale_color_manual(values=gp_duo4)+
  theme_minimal()+
  theme(legend.position = "bottom")+
  ylab("Density")+
  xlab("Value of the discriminant function on a review")+
  ylim(c(0,0.61))

ggsave(here("output", "figures", "density_lda_test.png"), plot = density_lda_test, width = 7, 
       height = 3, units = "in")
  
density_lda_test

```

```{r info_model_2}


predictions <- 
predictions %>% 
  as.tibble() %>% 
  mutate(class_2 = ifelse(x > 0, T, F))

conf_matrix <- 
cbind(fake = as.logical(test.data$fake),
      predict = as.logical(predictions$class_2)) %>%
  as_tibble() %>%
  dplyr::count(fake, predict) %>%
  dplyr::group_by(fake) %>%
  dplyr::mutate(percent = scales::percent(n/sum(n), accuracy =0.1))

confusionMatrix(predictions$class, test.data$fake)

# building roc curve
test.data.roc <- predict(model, newdata = test.data)
# Get the posteriors as a dataframe.
test.data.roc.posteriors <- as.data.frame(test.data.roc$posterior)
# Evaluate the model
pred <- ROCR::prediction(test.data.roc.posteriors[,2], test.data$fake)
roc.perf = ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- ROCR::performance(pred, measure = "auc")
auc.train <- auc.train@y.values

```

More precisely, the sensitivity and the specificity of the model are respectively `r conf_matrix[1,4]` and `r conf_matrix[4,4]`. It means that a genuine review has a `r conf_matrix[1,4]` chance of being predicted as genuine, and a fake review has a `r conf_matrix[4,4]` chance of being predicted as fake. This can however be played with, if we would want, for example, sensitivity to be higher (better accuracy on genuine reviews being predicted as genuine). However, it would be at the expense of specificity. Figure TKTK shows the receiver operating characteristic (ROC) curve of the model. True positive rate, on y axis, is the sensitivity. We could adapt the model to have a higher rate of fake reviews being predicted as fake, but it would be at the expense of genuine reviews being also more predicted as fake, or vice versa. The area under the curve (AUC) is an effective way to summarize the overall diagnostic accuracy of a model, between 0 and 1. The straight diagonal line would represent a model with a 0.5 AUC, suggesting that the model would have no discriminatory ability. For this particular model, the AUC is `r round(unlist(auc.train), digits = 2)`, which is considered acceptable. 

```{r roc_curve, include = T, fig.fullwidth=TRUE}

# Save plot to an object using a null PDF device
pdf(NULL)
dev.control(displaylist="enable")
plot(roc.perf, xlab = "False positive rate (1 - specificity)", ylab = "True positive rate (sensitivity)")
abline(a=0, b= 1)
text(x = .25, y = .7 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
roc_curve_plot <- recordPlot()
invisible(dev.off())

# Save
png(here("output","figures","roc_curve.png"), width = 600, height = 400)
roc_curve_plot
dev.off()

# Display the saved plot
grid::grid.newpage()
roc_curve_plot


```

```{r most_relevant_variables}

greatest_categories <- 
model$scaling %>%
  as_tibble() %>%
  mutate(value = (model$scaling %>% rownames()),
         p_n_impact = ifelse(LD1>0, "positive", "negative"),
         LD1 = abs(LD1)) %>% 
  arrange(-LD1)


```

The LIWC software generates, with a pre-defined dictionary, indicators by calculating the percentage of words, within the text provided, that match a psychologically meaningful category. The model gives a coefficients of linear discriminants to each of the categories. The most impactful categories on the model, the largest coefficients in their absolute value, is the use of the third person pronouns (she, he, they), the use of words related to time (end, until, season) and the use of the first person singular (I). The next ones are related to biological processes such as ingestion (dish, eat, pizza) or body (cheek, hands, spit). The perceptual processes of hearing (listen, hearing) is also an important coefficient.

## Results

With the assumption that a user letting a fake review will let more than one fake review, we gave scores to reviewers regarding their performance at the latest model. It also reduces the range of errors compared to analyzing each review individually. Some users were flagged to have a fraudulent tendency regarding their reviews, and this was used to give a score to listings. 

```{r overview of results}

per_user <- 
review_text_pred %>% 
  group_by(user_ID) %>% 
  summarize(mean_fake = mean(chance_fake, na.rm=T), 
            sd = sd(chance_fake, na.rm=T), nb_reviews = n())

fake_reviewers <- 
per_user %>% 
  filter(mean_fake-sd > 0, nb_reviews >= 5) %>% 
  pull(user_ID)

nb_fake_reviews <- 
review %>% 
  filter(user_ID %in% fake_reviewers, 
         property_ID %in% property$property_ID) %>% 
  nrow() %>% 
  prettyNum(",")

property_fake_reviewed <- 
review %>% 
  filter(user_ID %in% fake_reviewers) %>% 
  group_by(property_ID) %>% 
  summarize(nb_fake_review_from_fake_reviewers = n()) %>% 
  inner_join(property)

nb_hosts_fake_reviewed <- 
property_fake_reviewed %>% 
  distinct(host_ID) %>% nrow() %>% prettyNum(",")

```

The users we flagged to have a fraudulent tendency regarding their reviews were a. users who let 5 or more reviews we were able to analyze (of 5 words or more and in english) since their account creation, in any part of the world, and b. users for whom the mean of the result of all their reviews to the previous model, minus on time their standard deviation, is higher than 0 (a score higher than 0 is predicted by the model as being potentially fraudulent). This is a conservative operation, since following a normal distribution, it would mean the vast majority (84%) of a user's reviews were flagged as potentially fraudulent. We flagged `r prettyNum(length(fake_reviewers), ",")` users, who let `r nb_fake_reviews` reviews to `r prettyNum(nrow(property_fake_reviewed), ",")` properties in our dataset, owned by `r nb_hosts_fake_reviewed` hosts.

```{r nb_properties_fake_reviewed, include=T}

fake_reviewed_city <- 
property_fake_reviewed %>% 
  count(city) %>% 
  arrange(-n)

fake_reviewed_property_plot <- 
fake_reviewed_city %>% 
  ggplot()+
  geom_col(aes(city, n), fill = color_palette[3])+
  geom_text(aes(x = city, y = n, label=n), vjust=c(rep(2,9), -0.5))+
  theme_minimal()+
  xlab(NULL)+
  ylab("Number of properties")+
  theme(legend.position = "none")

ggsave(here("output", "figures", "fake_reviewed_property_plot.png"), plot = fake_reviewed_property_plot, width = 7.5, 
       height = 3, units = "in")

fake_reviewed_property_plot

```

FIGURE TKTK shows in which city there were the most properties reviewed by these flagged potential fraudulent reviewers. At the top of the list is Los Angeles and New York. Toronto is the third city with the highest amount of properties being reviewed by potential fraudulent reviewers. Surprisingly, for this indicator, Kissimmee and Davenport arrives last, and from a lot. This suggests that properties in these cities do not need to engage with fake reviewers. These two are cities with a very high rate of commercialization, and yet they engage much less with fake reviewers.

```{r fake_review_vs_commercial}

fake_review_freh <- 
property_fake_reviewed %>% 
  count(FREH) %>% 
  mutate(per = scales::percent(n/sum(n), accuracy = 0.1)) %>% 
  filter(FREH) %>% pull(per)

pop_freh <- 
  property %>% 
  count(FREH) %>% 
  mutate(per = scales::percent(n/sum(n), accuracy = 0.1)) %>% 
  filter(FREH) %>% pull(per)

k_d_freh <-  
  property %>% 
  filter(city %in% c("Kissimmee", "Davenport")) %>% 
  group_by(city) %>% 
  count(FREH)  %>% 
  mutate(per = scales::percent(n/sum(n), accuracy = 0.1)) %>% 
  filter(FREH)
  


```

Properties receiving reviews from these flagged users have a higher chance of being commercial than the rest of the population of properties. Indeed, `r fake_review_freh` of these properties were detected as being at least once frequently rented (reserved 90 days and available or reserved 180 days in a year). The figure is `r pop_freh` for the properties of the overall population. Standing above the average, Kissimmee and Davenport have `r k_d_freh[2,4]` and `r k_d_freh[1,4]` of their properties being frequently rented, but much fewer fake reviews. It seems like fraudulent reviewing is experienced by the largest, more common, STR markets.